{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PI-3 Running SFINCS using the National Water Model 3.0 inputs\n",
    "\n",
    "Below is a notebook for running SFINCS using National Water Model inputs over a domain of interest\n",
    "\n",
    "Author: Tadd Bindas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import dependencies and create national water model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely.ops import unary_union\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "import networkx as nx\n",
    "import zarr\n",
    "\n",
    "import hydromt\n",
    "from hydromt_sfincs import SfincsModel\n",
    "from hydromt_sfincs import utils\n",
    "from hydromt_sfincs import workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the model:\n",
    "\n",
    "First, we need to create a SFINCS Model object and some Data Catalogs to point at external data. The SFINCS object saves internal states and a config for mapping to external files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"/Users/taddbindas/projects/rise/data/SFINCS/ngwpc_data/\")\n",
    "yml_str = f\"\"\"\n",
    "meta:\n",
    "  root: {root.__str__()}\n",
    "  \n",
    "10m_lidar:\n",
    "  path: HUC6_110701_dem.tiff\n",
    "  data_type: RasterDataset\n",
    "  driver: raster\n",
    "  driver_kwargs:\n",
    "    chunks:\n",
    "      x: 6000\n",
    "      y: 6000\n",
    "  meta:\n",
    "    category: topography\n",
    "    crs: 5070\n",
    "  rename:\n",
    "    10m_lidar: elevtn\n",
    "\"\"\"\n",
    "data_lib = (\n",
    "    \"/Users/taddbindas/projects/rise/data/SFINCS/data_catalogs/10m_huc6_lidar.yml\"\n",
    ")\n",
    "with open(data_lib, mode=\"w\") as f:\n",
    "    f.write(yml_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_catalog = hydromt.DataCatalog(data_libs=[data_lib])\n",
    "sf = SfincsModel(data_libs=[data_lib], root=\"../data/ngwpc_data\", mode=\"w+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set up the bounding boxes around the catchments of interest\n",
    "\n",
    "Since we want to determine the flooding downstream of the current RFC station, we have to subset our graph to determine where these catchments exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "src_path = current_dir.parents[0]\n",
    "sys.path.append(src_path.__str__())\n",
    "\n",
    "from src.rise.utils import hydrofabric  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded HUC8\n",
    "huc_8 = \"11070103\"\n",
    "base_data_path = Path(\"/Users/taddbindas/projects/rise/data/NWM\")\n",
    "output_geojson = base_data_path / f\"output_{huc_8}.geojson\"\n",
    "huc8_data = base_data_path / \"huc8s_sp.gpkg\"\n",
    "gdf = gpd.read_file(huc8_data)\n",
    "subset  = gdf[gdf[\"HUC8\"] == huc_8]\n",
    "subset.to_file(output_geojson, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"/Users/taddbindas/hydrofabric/v20.1/gpkg/nextgen_11.gpkg\")\n",
    "\n",
    "flowlines = hydrofabric.get_layer(file_path, layer=\"flowpaths\")\n",
    "nexus = hydrofabric.get_layer(file_path, layer=\"nexus\")\n",
    "divides = hydrofabric.get_layer(file_path, layer=\"divides\")\n",
    "flowpath_attributes = hydrofabric.get_layer(file_path, layer=\"flowpath_attributes\")\n",
    "\n",
    "G = hydrofabric.get_hydrofabric_vpu_graph(nexus, flowlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded start and end nodes\n",
    "start_node = \"nex-2177032\"\n",
    "end_node = \"nex-2175887\"\n",
    "path = nx.shortest_path(G, start_node, end_node)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_flowlines = flowlines[\"id\"].isin(path) | flowlines[\"toid\"].isin(path)\n",
    "mask_nexus = nexus[\"id\"].isin(path)\n",
    "mask_divides = divides[\"id\"].isin(path)\n",
    "# mask_attributes = flowpath_attributes[\"id\"].isin(path)\n",
    "\n",
    "_subset_nexus = nexus[mask_nexus]\n",
    "_subset_flowlines = flowlines[mask_flowlines]\n",
    "_subset_divides = divides[mask_divides]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_subset_divides' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unary_union\n\u001b[0;32m----> 3\u001b[0m merged_polygon \u001b[38;5;241m=\u001b[39m unary_union(\u001b[43m_subset_divides\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m merged_gdf \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mGeoDataFrame(geometry\u001b[38;5;241m=\u001b[39m[merged_polygon], crs\u001b[38;5;241m=\u001b[39mgdf\u001b[38;5;241m.\u001b[39mcrs)\n\u001b[1;32m      5\u001b[0m output_divides \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/taddbindas/projects/rise/data/NWM/flowlines_divides.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_subset_divides' is not defined"
     ]
    }
   ],
   "source": [
    "merged_polygon = unary_union(_subset_divides['geometry'])\n",
    "merged_gdf = gpd.GeoDataFrame(geometry=[merged_polygon], crs=gdf.crs)\n",
    "output_divides = \"/Users/taddbindas/projects/rise/data/NWM/flowlines_divides.geojson\"\n",
    "merged_gdf.to_file(output_divides, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.setup_grid_from_region(\n",
    "    region = {'geom': 'tmp_ngwpc_data/coffeyville/flowlines_divides.geojson'},\n",
    "    res= 50,\n",
    "    rotated=True,\n",
    "    crs=_subset_divides.crs  # NAD83 / Conus Albers HARDCODED TODO figure out making this cleaner\n",
    ")\n",
    "# the input file is automatically updated. Uncomment to displayed below:\n",
    "print(sf.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load in Elevation Data\n",
    "\n",
    "We're going to be loading in elevation data from FIM-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dep = [{\"elevtn\": \"10m_lidar\", \"zmin\": 0.001}]\n",
    "\n",
    "dep = sf.setup_dep(datasets_dep=datasets_dep)\n",
    "\n",
    "_ = sf.plot_basemap(variable=\"dep\", bmap=\"sat\", zoomlevel=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Mask active cells, boundaries, and coasts\n",
    "\n",
    "We're going to be masking all active cells as cells above -5 meters. This will properly define coastal vs non-coastal boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing how to choose you active cells can be based on multiple criteria, here we only specify a minimum elevation of -5 meters\n",
    "sf.setup_mask_active(zmin=-5, reset_mask=True)\n",
    "\n",
    "# Make a plot of the mask file\n",
    "_ = sf.plot_basemap(variable=\"msk\", plot_bounds=True, bmap=\"sat\", zoomlevel=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Add output boundaries\n",
    "\n",
    "We're going to now mask the boundaries of catchments/rivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.setup_mask_bounds(btype=\"waterlevel\", zmax=-5, reset_bounds=True)\n",
    "\n",
    "# Here we add outflow cells, only where clicked in shapefile along part of the lateral boundaries\n",
    "sf.setup_mask_bounds(btype=\"outflow\", include_mask=merged_gdf, reset_bounds=True)\n",
    "\n",
    "# Make a plot of the mask file\n",
    "fig, ax = sf.plot_basemap(variable=\"msk\", plot_bounds=True, bmap=\"sat\", zoomlevel=12)\n",
    "merged_gdf.to_crs(sf.crs).boundary.plot(\n",
    "    ax=ax, color=\"k\", lw=1, ls=\"--\"\n",
    ")  # plot the shapefile given by the user as dashed line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Add river inflow data\n",
    "\n",
    "Using the hydrofabric we can add flowlines to show where rivers are located within the Hydrofabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.setup_river_inflow(\n",
    "    rivers=_subset_flowlines, keep_rivers_geom=True\n",
    ")\n",
    "print(sf.config)\n",
    "# Make a plot of model\n",
    "# note the src points and derived river network\n",
    "fig, ax = sf.plot_basemap(variable=\"dep\", plot_bounds=False, bmap=\"sat\", zoomlevel=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_riv = sf.geoms[\"rivers_inflow\"].copy()\n",
    "df_reordered = pd.merge(gdf_riv[\"id\"], flowpath_attributes, on='id', how='left')\n",
    "gdf_riv[\"rivwth\"] = np.mean(df_reordered[\"TopWdth\"].values) # width [m]\n",
    "\n",
    "# Assuming depth is 1.5m, TODO come back later\n",
    "gdf_riv[\"rivdph\"] = 1.5  # depth [m]\n",
    "gdf_riv[\"manning\"] = df_reordered[\"n\"].tolist()  # manning coefficient [s.m-1/3]\n",
    "gdf_riv[[\"geometry\", \"rivwth\", \"manning\"]]\n",
    "\n",
    "datasets_riv = [{\"centerlines\": gdf_riv}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Add spatially varying roughness\n",
    "\n",
    "Our roughness data needs to be spatially varying, and can use the flowpath attributes in the hydrofabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_riv_buf = gdf_riv.assign(geometry=gdf_riv.geometry.buffer(0.2))\n",
    "# da_manning = sf.grid.raster.rasterize(gdf_riv_buf, \"manning\", nodata=np.nan)\n",
    "da_manning = sf.grid.raster.rasterize(gdf_riv, \"manning\", nodata=np.nan)\n",
    "\n",
    "# use the river manning raster in combination with vito land to derive the manning roughness file\n",
    "# NOTE that we can combine in-memory data with data from the data catalog\n",
    "datasets_rgh = [{\"manning\": da_manning}]\n",
    "\n",
    "# uncomment to plot either the raster or the vector data:\n",
    "da_manning.plot(vmin=0, x='xc', y='yc', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Make subgrid derived tables:\n",
    "Subgrid derived tables are used to better capture the elevation and roughness of your domain, to either improve your results, or to allow you to run on a courser grid resolution (means faster simulation). For more info about subgrid tables, [click here](https://sfincs.readthedocs.io/en/latest/developments.html#recent-advancements-in-accuracy-subgrid-mode). \n",
    "\n",
    "You as user can specify multiple settings about how the subgrid derived tables should be made.\n",
    "\n",
    "Every single grid cell of the flux grid of the size inp.dx by inp.dy is defined into subgrid pixels (default nr_subgrid_pixels = 20).\n",
    "For every subgrid pixel the topobathy data is loaded, ideally this consists of high-resolution DEM datasets that you specify as user.\n",
    "\n",
    "In this example with dx=dy=50m, having nr_subgrid_pixels = 20 means we are loading data onto a 2.5 m subpixel grid\n",
    "However, the input data of Gebco and Merit_hydro is way coarser, therefore let's set the ratio to 5 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.setup_subgrid(\n",
    "    datasets_rgh=datasets_rgh,\n",
    "    datasets_dep=datasets_dep,\n",
    "    datasets_riv=datasets_riv,\n",
    "    nr_subgrid_pixels=5,\n",
    "    write_dep_tif=False,\n",
    "    write_man_tif=False,\n",
    ")\n",
    "_ = sf.plot_basemap(\n",
    "    variable=\"subgrid.z_zmin\", plot_bounds=False, bmap=\"sat\", zoomlevel=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Add an upstream discharge time-series as forcing (NWM retrospective)\n",
    "\n",
    "We're going to be pulling inputs from the national water model retrospective using TEEHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conus_df = pd.read_parquet(\"/Users/taddbindas/hydrofabric/v20.1/conus_net.parquet\")\n",
    "_df = conus_df[conus_df[\"id\"].isin(_subset_flowlines[\"id\"].values)]\n",
    "_feature_ids = _df[~np.isnan(_df[\"hf_id\"])]\n",
    "feature_ids = _feature_ids[\"hf_id\"].values.astype(int)\n",
    "mapping = {}\n",
    "for _hf_id, _id in zip(_feature_ids[\"hf_id\"], _feature_ids[\"id\"]):\n",
    "    _mapped_features = mapping.get(_id, None)\n",
    "    if _mapped_features is None:\n",
    "        mapping[_id] = [_hf_id]\n",
    "    else:\n",
    "        mapping[_id].append(_hf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teehr.loading.nwm import retrospective_points as nwm_retro\n",
    "from datetime import datetime\n",
    "# Define the parameters.\n",
    "NWM_VERSION = \"nwm30\"\n",
    "VARIABLE_NAME = \"streamflow\"\n",
    "START_DATE = datetime(2019, 5, 20)\n",
    "END_DATE = datetime(2019, 5, 28)\n",
    "LOCATION_IDS = feature_ids\n",
    "\n",
    "OUTPUT_ROOT = Path(\"/Users/taddbindas/projects/hydromt_sfincs/examples/tmp_ngwpc_data\")\n",
    "OUTPUT_DIR = Path(OUTPUT_ROOT, \"nwm30_retrospective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and load the data.\n",
    "nwm_retro.nwm_retro_to_parquet(\n",
    "    nwm_version=NWM_VERSION,\n",
    "    variable_name=VARIABLE_NAME,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    location_ids=LOCATION_IDS,\n",
    "    output_parquet_dir=OUTPUT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_data = pd.read_parquet(OUTPUT_DIR / \"20190520_20190528.parquet\")\n",
    "flow_data[['nwm_version', 'location_id']] = flow_data['location_id'].str.split('-', expand=True)\n",
    "flow_data[\"location_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = zarr.open_group(path=\"/Users/taddbindas/projects/hydromt_sfincs/examples/tmp_ngwpc_data\", mode=\"w\")\n",
    "flood_root = root.require_group(\"coffeyville.zarr\")\n",
    "flood_root.array(\n",
    "    name=\"value_time\",\n",
    "    data=np.array(flow_data[\"value_time\"].unique()),\n",
    "    dtype=\"datetime64[ns]\"\n",
    ")\n",
    "flood_root.tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in mapping.items():\n",
    "    v = np.array([str(int(_v)) for _v in v])\n",
    "    flow = flow_data[flow_data[\"location_id\"].isin(v)].groupby(\"value_time\")[\"value\"].mean().reset_index()[\"value\"].values\n",
    "    flood_root.array(\n",
    "        name=k,\n",
    "        data=flow,\n",
    "    )  \n",
    " \n",
    "flood_root.tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine source point catchment ID numbers for pulling zarr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_src = workflows.flwdir.river_source_points(\n",
    "    gdf_riv=_subset_flowlines,\n",
    "    gdf_mask=sf.region,\n",
    "    src_type=\"inflow\",\n",
    "    buffer=200,\n",
    "    river_upa=10.0,\n",
    "    river_len=1e3,\n",
    "    da_uparea=None,\n",
    "    reverse_river_geom=False,\n",
    "    logger=sf.logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "_subset_flowlines.plot(ax=ax, color='blue', linewidth=0.5, alpha=0.5, label='Flowlines')\n",
    "gdf_src.plot(ax=ax, color='red', markersize=30, label='Source Points')\n",
    "try:\n",
    "    cx.add_basemap(ax, crs=gdf_src.crs.to_string(), source=cx.providers.OpenStreetMap.Mapnik)\n",
    "except Exception as e:\n",
    "    print(f\"Couldn't add basemap: {e}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered_lines = _subset_flowlines.copy()\n",
    "buffered_lines['geometry'] = _subset_flowlines.geometry.buffer(50)\n",
    "intersection = gpd.overlay(gdf_src, buffered_lines, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = flood_root[\"value_time\"][:]\n",
    "dis = []\n",
    "for _id in intersection[\"id\"]:\n",
    "    dis.append(flood_root[_id][:])\n",
    "dis = np.array(dis).T\n",
    "\n",
    "index = sf.forcing[\"dis\"].index\n",
    "dispd = pd.DataFrame(index=time, columns=index, data=dis)\n",
    "# # now we call the function setup_discharge_forcing, which adds the discharge forcing to the src points\n",
    "sf.setup_discharge_forcing(timeseries=dispd)\n",
    "\n",
    "# # NOTE: the discharge forcing data is now stored in the sf.forcing dictionary\n",
    "sf.forcing.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Add weirfile:\n",
    "In SFINCS, a weirfile is often used to explicity account for line-element features such as dikes, dunes or floodwalls. Read more about structures in the [SFINCS manual](https://sfincs.readthedocs.io/en/latest/input_structures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weir_gdf = gpd.read_file(\"/Users/taddbindas/projects/hydromt_sfincs/examples/tmp_ngwpc_data/nld_subset_levees.gpkg\")\n",
    "weir_gdf.to_file(\"/Users/taddbindas/projects/hydromt_sfincs/examples/tmp_ngwpc_data/nld_subset_levees.geojson\", driver='GeoJSON')\n",
    "# In this example specify a 'line' style shapefile for the location of the weir to be added\n",
    "# NOTE: optional: dz argument - If provided, for weir structures the z value is calculated from the model elevation (dep) plus dz.\n",
    "sf.setup_structures(\n",
    "    structures=\"/Users/taddbindas/projects/hydromt_sfincs/examples/tmp_ngwpc_data/nld_subset_levees.geojson\",\n",
    "    stype=\"weir\",\n",
    "    dz=None,\n",
    ")\n",
    "\n",
    "# NOTE: the observation points are now stored in the sf.geoms dictionary\n",
    "sf.geoms.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Show and write model settings to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use predefined plotting function 'plot_basemap' to show your full SFINCS model setup\n",
    "_ = sf.plot_basemap(fn_out=\"basemap.png\", bmap=\"sat\", zoomlevel=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write()  # write all\n",
    "dir_list = os.listdir(sf.root)\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can proceed to the `run_sfincs_from_nwm.ipynb` to run and visualize this flood model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
